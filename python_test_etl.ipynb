{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this Python file is to create and test code for etl\n",
    "### We will first use the data (already unzipped from existing zip files) in sub-folders in data folder\n",
    "#### song-data   - input data store for songs\n",
    "#### log-data    - input data store for logs\n",
    "#### output-data - output data folder from program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SONG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_SONG_WORKSPACE']\n",
    "#LOG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_LOG_WORKSPACE']\n",
    "OUTPUT_DATA_LOCAL=config['LOCAL']['OUTPUT_DATA_WORKSPACE']\n",
    "SONG_DATA_LOCAL=config['LOCAL']['INPUT_DATA']\n",
    "LOG_DATA_LOCAL=config['LOCAL']['INPUT_DATA']\n",
    "input_data=config['LOCAL']['INPUT_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-dend/song_data/A/A/A/*.json\n"
     ]
    }
   ],
   "source": [
    "# Read local song_data\n",
    "song_data_path = SONG_DATA_LOCAL\n",
    "\n",
    "# Use this instead if you want to read song_data from S3.\n",
    "#song_data_path = INPUT_DATA_SD\n",
    "song_data = f'{input_data}/song_data/*/*/*/*.json'\n",
    "log_data = '{input_data}/log_data/'\n",
    "\n",
    "#for output: df.write.parquet(\"s3a://hs-output-data\",mode=\"overwrite\")\n",
    "song_data = os.path.join(input_data, \"song_data/A/A/A/*.json\")\n",
    "print(song_data)\n",
    "df = spark.read.json(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+---------------------------------+----------------+-------------------------+---------+---------+------------------+------------------------------------------------------+----+\n",
      "|artist_id         |artist_latitude|artist_location                  |artist_longitude|artist_name              |duration |num_songs|song_id           |title                                                 |year|\n",
      "+------------------+---------------+---------------------------------+----------------+-------------------------+---------+---------+------------------+------------------------------------------------------+----+\n",
      "|ARTC1LV1187B9A4858|51.4536        |Goldsmith's College, Lewisham, Lo|-0.01802        |The Bonzo Dog Band       |301.40036|1        |SOAFBCP12A8C13CC7D|King Of Scurf (2007 Digital Remaster)                 |1972|\n",
      "|ARA23XO1187B9AF18F|40.57885       |Carteret, New Jersey             |-74.21956       |The Smithereens          |192.522  |1        |SOKTJDS12AF72A25E5|Drown In My Own Tears (24-Bit Digitally Remastered 04)|0   |\n",
      "|ARSVTNL1187B992A91|51.50632       |London, England                  |-0.12714        |Jonathan King            |129.85424|1        |SOEKAZG12AB018837E|I'll Slap Your Face (Entertainment USA Theme)         |2001|\n",
      "|AR73AIO1187B9AD57B|37.77916       |San Francisco, CA                |-122.42005      |Western Addiction        |118.07302|1        |SOQPWCR12A6D4FB2A3|A Poor Recipe For Civic Cohesion                      |2005|\n",
      "|ARXQBR11187B98A2CC|null           |Liverpool, England               |null            |Frankie Goes To Hollywood|821.05424|1        |SOBRKGM12A8C139EF6|Welcome to the Pleasuredome                           |1985|\n",
      "+------------------+---------------+---------------------------------+----------------+-------------------------+---------+---------+------------------+------------------------------------------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs table schema: \n",
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract columns to create songs table\n",
    "df.createOrReplaceTempView(\"songs_table_df\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM songs_table_df\n",
    "    ORDER BY song_id\n",
    "\"\"\")\n",
    "print(\"Songs table schema: \")\n",
    "songs_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "output_data = OUTPUT_DATA_LOCAL\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = output_data + \"songs_table.parquet\" + \"_\" + now\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_id\").parquet(songs_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"artists_table_df\")\n",
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT  artist_id        AS artist_id,\n",
    "            artist_name      AS name,\n",
    "            artist_location  AS location,\n",
    "            artist_latitude  AS latitude,\n",
    "            artist_longitude AS longitude\n",
    "    FROM artists_table_df\n",
    "    ORDER by artist_id desc\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+------------------+-------------------------+------------------+--------+----------+\n",
      "|artist_id         |name                     |location          |latitude|longitude |\n",
      "+------------------+-------------------------+------------------+--------+----------+\n",
      "|ARZKCQM1257509D107|Dataphiles               |                  |null    |null      |\n",
      "|ARZ5H0P1187B98A1DD|Snoop Dogg               |Long Beach, CA    |33.76672|-118.1924 |\n",
      "|ARY589G1187B9A9F4E|Talkdemonic              |Portland, OR      |45.51179|-122.67563|\n",
      "|ARXR32B1187FB57099|Gob                      |                  |null    |null      |\n",
      "|ARXQBR11187B98A2CC|Frankie Goes To Hollywood|Liverpool, England|null    |null      |\n",
      "+------------------+-------------------------+------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.printSchema()\n",
    "artists_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "artists_table_path = OUTPUT_DATA_LOCAL + \"artists_table.parquet\" + \"_\" + now\n",
    "artists_table.write.parquet(artists_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data =LOG_DATA_LOCAL\n",
    "\n",
    "# read log data file\n",
    "#df_log_data = spark.read.json(log_data)\n",
    "df_log_data = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*\")\n",
    "# filter by actions for song plays\n",
    "df_log_data_filtered = df_log_data.filter(df_log_data.page == 'NextSong') \n",
    "\n",
    "# extract columns for users table    \n",
    "df_log_data_filtered.createOrReplaceTempView(\"users_table_df\")\n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT userId    AS user_id,\n",
    "                    firstName AS first_name,\n",
    "                    lastName  AS last_name,\n",
    "                    gender,\n",
    "                    level\n",
    "    FROM users_table_df\n",
    "    ORDER BY last_name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table.printSchema()\n",
    "users_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write users table to parquet files\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "users_table_path = OUTPUT_DATA_LOCAL + \"users_table.parquet\" + \"_\" + now\n",
    "users_table.write.parquet(users_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column with timestamp\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "@udf(t.TimestampType())\n",
    "def get_timestamp (ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0)\n",
    "\n",
    "#add column timestamp, and use results of user-defined-function above to populate it\n",
    "df_log_data_filtered = df_log_data_filtered.withColumn(\"timestamp\", get_timestamp(\"ts\"))\n",
    "df_log_data_filtered.printSchema()\n",
    "df_log_data_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column with datetime\n",
    "@udf(t.StringType())\n",
    "def get_datetime(ts):\n",
    "    return datetime.fromtimestamp(ts /1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_log_data_filtered = df_log_data_filtered.withColumn(\"datetime\", get_datetime(\"ts\"))\n",
    "df_log_data_filtered.printSchema()\n",
    "df_log_data_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create time table\n",
    "df_log_data_filtered.createOrReplaceTempView(\"time_table_df\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT datetime AS start_time, \n",
    "                     hour(timestamp) AS hour, \n",
    "                     day(timestamp)  AS day, \n",
    "                     weekofyear(timestamp) AS week,\n",
    "                     month(timestamp) AS month,\n",
    "                     year(timestamp) AS year,\n",
    "                     dayofweek(timestamp) AS weekday\n",
    "    FROM time_table_df\n",
    "    ORDER BY start_time\n",
    "\"\"\")\n",
    "time_table.printSchema()\n",
    "time_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write time table to parquet file\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "time_table_path = OUTPUT_DATA_LOCAL + \"time_table.parquet\" + \"_\" + now\n",
    "print(time_table_path)\n",
    "time_table.write.parquet(time_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create songplays table\n",
    "#join song data and artists table for log_data.artist = song data's artist & song = song table's title\n",
    "df_joined = df_log_data_filtered.join(df, (df_log_data_filtered.artist == df.artist_name) & \\\n",
    "                                          (df_log_data_filtered.song == df.title))\n",
    "df_joined.printSchema()\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column to df and create table\n",
    "df_joined = df_joined.withColumn(\"songplay_id\", f.monotonically_increasing_id())\n",
    "\n",
    "df_joined.createOrReplaceTempView(\"songplays_table_df\")\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT  songplay_id AS songplay_id, \n",
    "            timestamp   AS start_time, \n",
    "            userId      AS user_id, \n",
    "            level       AS level,\n",
    "            song_id     AS song_id,\n",
    "            artist_id   AS artist_id,\n",
    "            sessionId   AS session_id,\n",
    "            location    AS location,\n",
    "            userAgent   AS user_agent\n",
    "    FROM songplays_table_df\n",
    "    ORDER BY (user_id, session_id)\n",
    "\"\"\")\n",
    "\n",
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write songplays table to parquet files\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songplays_table_path = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\" + \"_\" + now\n",
    "print(\"Writing output to: \", songplays_table_path)\n",
    "songplays_table.write.parquet(songplays_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
